{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Wendyuf/ColorGNN/blob/main/docs/Interacting_with_open_clip.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YPHN7PJgKOzb"
      },
      "source": [
        "# Interacting with open_clip\n",
        "\n",
        "This is a self-contained notebook that shows how to download and run open_clip models, calculate the similarity between arbitrary image and text inputs, and perform zero-shot image classifications."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z7F87xdw9rBX"
      },
      "source": [
        "## Preparation for colab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "ck9qjK8z9rBX"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0BpdJkdBssk9",
        "outputId": "5ffb1547-733a-4044-e7b8-52217a42132c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: open_clip_torch in /usr/local/lib/python3.11/dist-packages (2.32.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Requirement already satisfied: torch>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from open_clip_torch) (2.6.0+cu124)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (from open_clip_torch) (0.21.0+cu124)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.11/dist-packages (from open_clip_torch) (2024.11.6)\n",
            "Requirement already satisfied: ftfy in /usr/local/lib/python3.11/dist-packages (from open_clip_torch) (6.3.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from open_clip_torch) (4.67.1)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.11/dist-packages (from open_clip_torch) (0.33.4)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.11/dist-packages (from open_clip_torch) (0.5.3)\n",
            "Requirement already satisfied: timm in /usr/local/lib/python3.11/dist-packages (from open_clip_torch) (1.0.17)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.58.5)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: numpy>=1.23 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.2.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.9.0.post0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.9.0->open_clip_torch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9.0->open_clip_torch) (4.14.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.9.0->open_clip_torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9.0->open_clip_torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.9.0->open_clip_torch) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9.0->open_clip_torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9.0->open_clip_torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9.0->open_clip_torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9.0->open_clip_torch) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9.0->open_clip_torch) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9.0->open_clip_torch) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9.0->open_clip_torch) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9.0->open_clip_torch) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9.0->open_clip_torch) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9.0->open_clip_torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9.0->open_clip_torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9.0->open_clip_torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9.0->open_clip_torch) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9.0->open_clip_torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9.0->open_clip_torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.9.0->open_clip_torch) (1.3.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from ftfy->open_clip_torch) (0.2.13)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub->open_clip_torch) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub->open_clip_torch) (2.32.3)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub->open_clip_torch) (1.1.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.9.0->open_clip_torch) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub->open_clip_torch) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub->open_clip_torch) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub->open_clip_torch) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub->open_clip_torch) (2025.7.14)\n"
          ]
        }
      ],
      "source": [
        "! pip install open_clip_torch matplotlib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "C1hkDT38hSaP"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eFxgLV5HAEEw"
      },
      "source": [
        "# Loading the model\n",
        "\n",
        "`clip.available_models()` will list the names of available CLIP models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uLFS29hnhlY4",
        "outputId": "e6967fa5-7665-4df2-d3b6-a50bbd2fb48e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('RN50', 'openai'),\n",
              " ('RN50', 'yfcc15m'),\n",
              " ('RN50', 'cc12m'),\n",
              " ('RN101', 'openai'),\n",
              " ('RN101', 'yfcc15m'),\n",
              " ('RN50x4', 'openai'),\n",
              " ('RN50x16', 'openai'),\n",
              " ('RN50x64', 'openai'),\n",
              " ('ViT-B-32', 'openai'),\n",
              " ('ViT-B-32', 'laion400m_e31'),\n",
              " ('ViT-B-32', 'laion400m_e32'),\n",
              " ('ViT-B-32', 'laion2b_e16'),\n",
              " ('ViT-B-32', 'laion2b_s34b_b79k'),\n",
              " ('ViT-B-32', 'datacomp_xl_s13b_b90k'),\n",
              " ('ViT-B-32', 'datacomp_m_s128m_b4k'),\n",
              " ('ViT-B-32', 'commonpool_m_clip_s128m_b4k'),\n",
              " ('ViT-B-32', 'commonpool_m_laion_s128m_b4k'),\n",
              " ('ViT-B-32', 'commonpool_m_image_s128m_b4k'),\n",
              " ('ViT-B-32', 'commonpool_m_text_s128m_b4k'),\n",
              " ('ViT-B-32', 'commonpool_m_basic_s128m_b4k'),\n",
              " ('ViT-B-32', 'commonpool_m_s128m_b4k'),\n",
              " ('ViT-B-32', 'datacomp_s_s13m_b4k'),\n",
              " ('ViT-B-32', 'commonpool_s_clip_s13m_b4k'),\n",
              " ('ViT-B-32', 'commonpool_s_laion_s13m_b4k'),\n",
              " ('ViT-B-32', 'commonpool_s_image_s13m_b4k'),\n",
              " ('ViT-B-32', 'commonpool_s_text_s13m_b4k'),\n",
              " ('ViT-B-32', 'commonpool_s_basic_s13m_b4k'),\n",
              " ('ViT-B-32', 'commonpool_s_s13m_b4k'),\n",
              " ('ViT-B-32', 'metaclip_400m'),\n",
              " ('ViT-B-32', 'metaclip_fullcc'),\n",
              " ('ViT-B-32-256', 'datacomp_s34b_b86k'),\n",
              " ('ViT-B-16', 'openai'),\n",
              " ('ViT-B-16', 'laion400m_e31'),\n",
              " ('ViT-B-16', 'laion400m_e32'),\n",
              " ('ViT-B-16', 'laion2b_s34b_b88k'),\n",
              " ('ViT-B-16', 'datacomp_xl_s13b_b90k'),\n",
              " ('ViT-B-16', 'datacomp_l_s1b_b8k'),\n",
              " ('ViT-B-16', 'commonpool_l_clip_s1b_b8k'),\n",
              " ('ViT-B-16', 'commonpool_l_laion_s1b_b8k'),\n",
              " ('ViT-B-16', 'commonpool_l_image_s1b_b8k'),\n",
              " ('ViT-B-16', 'commonpool_l_text_s1b_b8k'),\n",
              " ('ViT-B-16', 'commonpool_l_basic_s1b_b8k'),\n",
              " ('ViT-B-16', 'commonpool_l_s1b_b8k'),\n",
              " ('ViT-B-16', 'dfn2b'),\n",
              " ('ViT-B-16', 'metaclip_400m'),\n",
              " ('ViT-B-16', 'metaclip_fullcc'),\n",
              " ('ViT-B-16-plus-240', 'laion400m_e31'),\n",
              " ('ViT-B-16-plus-240', 'laion400m_e32'),\n",
              " ('ViT-L-14', 'openai'),\n",
              " ('ViT-L-14', 'laion400m_e31'),\n",
              " ('ViT-L-14', 'laion400m_e32'),\n",
              " ('ViT-L-14', 'laion2b_s32b_b82k'),\n",
              " ('ViT-L-14', 'datacomp_xl_s13b_b90k'),\n",
              " ('ViT-L-14', 'commonpool_xl_clip_s13b_b90k'),\n",
              " ('ViT-L-14', 'commonpool_xl_laion_s13b_b90k'),\n",
              " ('ViT-L-14', 'commonpool_xl_s13b_b90k'),\n",
              " ('ViT-L-14', 'metaclip_400m'),\n",
              " ('ViT-L-14', 'metaclip_fullcc'),\n",
              " ('ViT-L-14', 'dfn2b'),\n",
              " ('ViT-L-14', 'dfn2b_s39b'),\n",
              " ('ViT-L-14-336', 'openai'),\n",
              " ('ViT-H-14', 'laion2b_s32b_b79k'),\n",
              " ('ViT-H-14', 'metaclip_fullcc'),\n",
              " ('ViT-H-14', 'metaclip_altogether'),\n",
              " ('ViT-H-14', 'dfn5b'),\n",
              " ('ViT-H-14-378', 'dfn5b'),\n",
              " ('ViT-g-14', 'laion2b_s12b_b42k'),\n",
              " ('ViT-g-14', 'laion2b_s34b_b88k'),\n",
              " ('ViT-bigG-14', 'laion2b_s39b_b160k'),\n",
              " ('ViT-bigG-14', 'metaclip_fullcc'),\n",
              " ('roberta-ViT-B-32', 'laion2b_s12b_b32k'),\n",
              " ('xlm-roberta-base-ViT-B-32', 'laion5b_s13b_b90k'),\n",
              " ('xlm-roberta-large-ViT-H-14', 'frozen_laion5b_s13b_b90k'),\n",
              " ('convnext_base', 'laion400m_s13b_b51k'),\n",
              " ('convnext_base_w', 'laion2b_s13b_b82k'),\n",
              " ('convnext_base_w', 'laion2b_s13b_b82k_augreg'),\n",
              " ('convnext_base_w', 'laion_aesthetic_s13b_b82k'),\n",
              " ('convnext_base_w_320', 'laion_aesthetic_s13b_b82k'),\n",
              " ('convnext_base_w_320', 'laion_aesthetic_s13b_b82k_augreg'),\n",
              " ('convnext_large_d', 'laion2b_s26b_b102k_augreg'),\n",
              " ('convnext_large_d_320', 'laion2b_s29b_b131k_ft'),\n",
              " ('convnext_large_d_320', 'laion2b_s29b_b131k_ft_soup'),\n",
              " ('convnext_xxlarge', 'laion2b_s34b_b82k_augreg'),\n",
              " ('convnext_xxlarge', 'laion2b_s34b_b82k_augreg_rewind'),\n",
              " ('convnext_xxlarge', 'laion2b_s34b_b82k_augreg_soup'),\n",
              " ('coca_ViT-B-32', 'laion2b_s13b_b90k'),\n",
              " ('coca_ViT-B-32', 'mscoco_finetuned_laion2b_s13b_b90k'),\n",
              " ('coca_ViT-L-14', 'laion2b_s13b_b90k'),\n",
              " ('coca_ViT-L-14', 'mscoco_finetuned_laion2b_s13b_b90k'),\n",
              " ('EVA01-g-14', 'laion400m_s11b_b41k'),\n",
              " ('EVA01-g-14-plus', 'merged2b_s11b_b114k'),\n",
              " ('EVA02-B-16', 'merged2b_s8b_b131k'),\n",
              " ('EVA02-L-14', 'merged2b_s4b_b131k'),\n",
              " ('EVA02-L-14-336', 'merged2b_s6b_b61k'),\n",
              " ('EVA02-E-14', 'laion2b_s4b_b115k'),\n",
              " ('EVA02-E-14-plus', 'laion2b_s9b_b144k'),\n",
              " ('ViT-B-16-SigLIP', 'webli'),\n",
              " ('ViT-B-16-SigLIP-256', 'webli'),\n",
              " ('ViT-B-16-SigLIP-i18n-256', 'webli'),\n",
              " ('ViT-B-16-SigLIP-384', 'webli'),\n",
              " ('ViT-B-16-SigLIP-512', 'webli'),\n",
              " ('ViT-L-16-SigLIP-256', 'webli'),\n",
              " ('ViT-L-16-SigLIP-384', 'webli'),\n",
              " ('ViT-SO400M-14-SigLIP', 'webli'),\n",
              " ('ViT-SO400M-16-SigLIP-i18n-256', 'webli'),\n",
              " ('ViT-SO400M-14-SigLIP-378', 'webli'),\n",
              " ('ViT-SO400M-14-SigLIP-384', 'webli'),\n",
              " ('ViT-B-32-SigLIP2-256', 'webli'),\n",
              " ('ViT-B-16-SigLIP2', 'webli'),\n",
              " ('ViT-B-16-SigLIP2-256', 'webli'),\n",
              " ('ViT-B-16-SigLIP2-384', 'webli'),\n",
              " ('ViT-B-16-SigLIP2-512', 'webli'),\n",
              " ('ViT-L-16-SigLIP2-256', 'webli'),\n",
              " ('ViT-L-16-SigLIP2-384', 'webli'),\n",
              " ('ViT-L-16-SigLIP2-512', 'webli'),\n",
              " ('ViT-SO400M-14-SigLIP2', 'webli'),\n",
              " ('ViT-SO400M-14-SigLIP2-378', 'webli'),\n",
              " ('ViT-SO400M-16-SigLIP2-256', 'webli'),\n",
              " ('ViT-SO400M-16-SigLIP2-384', 'webli'),\n",
              " ('ViT-SO400M-16-SigLIP2-512', 'webli'),\n",
              " ('ViT-gopt-16-SigLIP2-256', 'webli'),\n",
              " ('ViT-gopt-16-SigLIP2-384', 'webli'),\n",
              " ('ViT-L-14-CLIPA', 'datacomp1b'),\n",
              " ('ViT-L-14-CLIPA-336', 'datacomp1b'),\n",
              " ('ViT-H-14-CLIPA', 'datacomp1b'),\n",
              " ('ViT-H-14-CLIPA-336', 'laion2b'),\n",
              " ('ViT-H-14-CLIPA-336', 'datacomp1b'),\n",
              " ('ViT-bigG-14-CLIPA', 'datacomp1b'),\n",
              " ('ViT-bigG-14-CLIPA-336', 'datacomp1b'),\n",
              " ('nllb-clip-base', 'v1'),\n",
              " ('nllb-clip-large', 'v1'),\n",
              " ('nllb-clip-base-siglip', 'v1'),\n",
              " ('nllb-clip-base-siglip', 'mrl'),\n",
              " ('nllb-clip-large-siglip', 'v1'),\n",
              " ('nllb-clip-large-siglip', 'mrl'),\n",
              " ('MobileCLIP-S1', 'datacompdr'),\n",
              " ('MobileCLIP-S2', 'datacompdr'),\n",
              " ('MobileCLIP-B', 'datacompdr'),\n",
              " ('MobileCLIP-B', 'datacompdr_lt'),\n",
              " ('ViTamin-S', 'datacomp1b'),\n",
              " ('ViTamin-S-LTT', 'datacomp1b'),\n",
              " ('ViTamin-B', 'datacomp1b'),\n",
              " ('ViTamin-B-LTT', 'datacomp1b'),\n",
              " ('ViTamin-L', 'datacomp1b'),\n",
              " ('ViTamin-L-256', 'datacomp1b'),\n",
              " ('ViTamin-L-336', 'datacomp1b'),\n",
              " ('ViTamin-L-384', 'datacomp1b'),\n",
              " ('ViTamin-L2', 'datacomp1b'),\n",
              " ('ViTamin-L2-256', 'datacomp1b'),\n",
              " ('ViTamin-L2-336', 'datacomp1b'),\n",
              " ('ViTamin-L2-384', 'datacomp1b'),\n",
              " ('ViTamin-XL-256', 'datacomp1b'),\n",
              " ('ViTamin-XL-336', 'datacomp1b'),\n",
              " ('ViTamin-XL-384', 'datacomp1b'),\n",
              " ('RN50-quickgelu', 'openai'),\n",
              " ('RN50-quickgelu', 'yfcc15m'),\n",
              " ('RN50-quickgelu', 'cc12m'),\n",
              " ('RN101-quickgelu', 'openai'),\n",
              " ('RN101-quickgelu', 'yfcc15m'),\n",
              " ('RN50x4-quickgelu', 'openai'),\n",
              " ('RN50x16-quickgelu', 'openai'),\n",
              " ('RN50x64-quickgelu', 'openai'),\n",
              " ('ViT-B-32-quickgelu', 'openai'),\n",
              " ('ViT-B-32-quickgelu', 'laion400m_e31'),\n",
              " ('ViT-B-32-quickgelu', 'laion400m_e32'),\n",
              " ('ViT-B-32-quickgelu', 'metaclip_400m'),\n",
              " ('ViT-B-32-quickgelu', 'metaclip_fullcc'),\n",
              " ('ViT-B-16-quickgelu', 'openai'),\n",
              " ('ViT-B-16-quickgelu', 'dfn2b'),\n",
              " ('ViT-B-16-quickgelu', 'metaclip_400m'),\n",
              " ('ViT-B-16-quickgelu', 'metaclip_fullcc'),\n",
              " ('ViT-L-14-quickgelu', 'openai'),\n",
              " ('ViT-L-14-quickgelu', 'metaclip_400m'),\n",
              " ('ViT-L-14-quickgelu', 'metaclip_fullcc'),\n",
              " ('ViT-L-14-quickgelu', 'dfn2b'),\n",
              " ('ViT-L-14-336-quickgelu', 'openai'),\n",
              " ('ViT-H-14-quickgelu', 'metaclip_fullcc'),\n",
              " ('ViT-H-14-quickgelu', 'dfn5b'),\n",
              " ('ViT-H-14-378-quickgelu', 'dfn5b'),\n",
              " ('ViT-bigG-14-quickgelu', 'metaclip_fullcc')]"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "import open_clip\n",
        "open_clip.list_pretrained()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "8pAHe7nD9rBa"
      },
      "outputs": [],
      "source": [
        "model, _, preprocess = open_clip.create_model_and_transforms('convnext_base_w', pretrained='laion2b_s13b_b82k_augreg')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IBRVTY9lbGm8",
        "outputId": "6d73b8b0-61ff-455e-c05d-42eee7e3497a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model parameters: 179,385,345\n",
            "Context length: 77\n",
            "Vocab size: 49408\n"
          ]
        }
      ],
      "source": [
        "model.eval()\n",
        "context_length = model.context_length\n",
        "vocab_size = model.vocab_size\n",
        "\n",
        "print(\"Model parameters:\", f\"{np.sum([int(np.prod(p.shape)) for p in model.parameters()]):,}\")\n",
        "print(\"Context length:\", context_length)\n",
        "print(\"Vocab size:\", vocab_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "21slhZGCqANb"
      },
      "source": [
        "# Image Preprocessing\n",
        "\n",
        "We resize the input images and center-crop them to conform with the image resolution that the model expects. Before doing so, we will normalize the pixel intensity using the dataset mean and standard deviation.\n",
        "\n",
        "The second return value from `clip.load()` contains a torchvision `Transform` that performs this preprocessing.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d6cpiIFHp9N6",
        "outputId": "0c17baa4-5bc5-4f3e-8fdf-006f4a7def64"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Compose(\n",
              "    Resize(size=256, interpolation=bicubic, max_size=None, antialias=True)\n",
              "    CenterCrop(size=(256, 256))\n",
              "    <function _convert_to_rgb at 0x7e9b99617b00>\n",
              "    ToTensor()\n",
              "    Normalize(mean=(0.48145466, 0.4578275, 0.40821073), std=(0.26862954, 0.26130258, 0.27577711))\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "preprocess"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xwSB5jZki3Cj"
      },
      "source": [
        "# Text Preprocessing\n",
        "\n",
        "We use a case-insensitive tokenizer, which can be invoked using `tokenizer.tokenize()`. By default, the outputs are padded to become 77 tokens long, which is what the CLIP models expects."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "8f9coA_O9rBb"
      },
      "outputs": [],
      "source": [
        "from open_clip import tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qGom156-i2kL",
        "outputId": "72c6a870-02eb-4181-fac6-e2bcab262325"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[49406,  3306,  1002,   256, 49407,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0]])"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ],
      "source": [
        "tokenizer.tokenize(\"Hello World!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4W8ARJVqBJXs"
      },
      "source": [
        "# Setting up input images and texts\n",
        "\n",
        "We are going to feed 8 example images and their textual descriptions to the model, and compare the similarity between the corresponding features.\n",
        "\n",
        "The tokenizer is case-insensitive, and we can freely give any suitable textual descriptions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "tMc1AXzBlhzm"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import skimage\n",
        "import IPython.display\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "\n",
        "from collections import OrderedDict\n",
        "import torch\n",
        "\n",
        "%matplotlib inline\n",
        "%config InlineBackend.figure_format = 'retina'\n",
        "\n",
        "# images in skimage to use and their textual descriptions\n",
        "descriptions = {\n",
        "    \"page\": \"a page of text about segmentation\",\n",
        "    \"chelsea\": \"a facial photo of a tabby cat\",\n",
        "    \"astronaut\": \"a portrait of an astronaut with the American flag\",\n",
        "    \"rocket\": \"a rocket standing on a launchpad\",\n",
        "    \"motorcycle_right\": \"a red motorcycle standing in a garage\",\n",
        "    \"camera\": \"a person looking at a camera on a tripod\",\n",
        "    \"horse\": \"a black-and-white silhouette of a horse\",\n",
        "    \"coffee\": \"a cup of coffee on a saucer\",\n",
        "    \"1\": \"a leather with a defect\"\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 349
        },
        "id": "NSSrLY185jSf",
        "outputId": "71ae4b1b-f95f-40a9-c0a9-8d28e9b8b580"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/root/.cache/scikit-image/0.25.2/data/1.png'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-51-1596377289.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mskimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"RGB\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/PIL/Image.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(fp, mode, formats)\u001b[0m\n\u001b[1;32m   3503\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3504\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3505\u001b[0;31m         \u001b[0mfp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuiltins\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3506\u001b[0m         \u001b[0mexclusive_fp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3507\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/root/.cache/scikit-image/0.25.2/data/1.png'"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1600x500 with 0 Axes>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "original_images = []\n",
        "images = []\n",
        "texts = []\n",
        "plt.figure(figsize=(16, 5))\n",
        "\n",
        "for filename in [filename for filename in os.listdir(\"/content/s\") if filename.endswith(\".png\") or filename.endswith(\".jpg\")]:\n",
        "    name = os.path.splitext(filename)[0]\n",
        "    if name not in descriptions:\n",
        "        continue\n",
        "\n",
        "    image = Image.open(os.path.join(\"/content/s\", filename)).convert(\"RGB\")\n",
        "\n",
        "    plt.subplot(2, 4, len(images) + 1)\n",
        "    plt.imshow(image)\n",
        "    plt.title(f\"{filename}\\n{descriptions[name]}\")\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])\n",
        "\n",
        "    original_images.append(image)\n",
        "    images.append(preprocess(image))\n",
        "    texts.append(descriptions[name])\n",
        "print([filename for filename in os.listdir(\"/content/s\") if filename.endswith(\".png\") or filename.endswith(\".jpg\")])\n",
        "plt.tight_layout()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WEVKsji6WOIX"
      },
      "source": [
        "## Building features\n",
        "\n",
        "We normalize the images, tokenize each text input, and run the forward pass of the model to get the image and text features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "HBgCanxi8JKw",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 280
        },
        "outputId": "8afcec53-f63b-4dd8-fab6-b4dbf0528a46"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "need at least one array to stack",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-49-2557629113.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mimage_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtext_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"This is \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mdesc\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdesc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtexts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/numpy/_core/shape_base.py\u001b[0m in \u001b[0;36mstack\u001b[0;34m(arrays, axis, out, dtype, casting)\u001b[0m\n\u001b[1;32m    442\u001b[0m     \u001b[0marrays\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0masanyarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0marr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    443\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 444\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'need at least one array to stack'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    445\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    446\u001b[0m     \u001b[0mshapes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0marr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: need at least one array to stack"
          ]
        }
      ],
      "source": [
        "image_input = torch.tensor(np.stack(images))\n",
        "text_tokens = tokenizer.tokenize([\"This is \" + desc for desc in texts])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZN9I0nIBZ_vW"
      },
      "outputs": [],
      "source": [
        "with torch.no_grad():\n",
        "    image_features = model.encode_image(image_input).float()\n",
        "    text_features = model.encode_text(text_tokens).float()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cuxm2Gt4Wvzt"
      },
      "source": [
        "## Calculating cosine similarity\n",
        "\n",
        "We normalize the features and calculate the dot product of each pair."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yKAxkQR7bf3A"
      },
      "outputs": [],
      "source": [
        "image_features /= image_features.norm(dim=-1, keepdim=True)\n",
        "text_features /= text_features.norm(dim=-1, keepdim=True)\n",
        "similarity = text_features.cpu().numpy() @ image_features.cpu().numpy().T"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C5zvMxh8cU6m"
      },
      "outputs": [],
      "source": [
        "count = len(descriptions)\n",
        "\n",
        "plt.figure(figsize=(20, 14))\n",
        "plt.imshow(similarity, vmin=0.1, vmax=0.3)\n",
        "# plt.colorbar()\n",
        "plt.yticks(range(count), texts, fontsize=18)\n",
        "plt.xticks([])\n",
        "for i, image in enumerate(original_images):\n",
        "    plt.imshow(image, extent=(i - 0.5, i + 0.5, -1.6, -0.6), origin=\"lower\")\n",
        "for x in range(similarity.shape[1]):\n",
        "    for y in range(similarity.shape[0]):\n",
        "        plt.text(x, y, f\"{similarity[y, x]:.2f}\", ha=\"center\", va=\"center\", size=12)\n",
        "\n",
        "for side in [\"left\", \"top\", \"right\", \"bottom\"]:\n",
        "  plt.gca().spines[side].set_visible(False)\n",
        "\n",
        "plt.xlim([-0.5, count - 0.5])\n",
        "plt.ylim([count + 0.5, -2])\n",
        "\n",
        "plt.title(\"Cosine similarity between text and image features\", size=20)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "alePijoXy6AH"
      },
      "source": [
        "# Zero-Shot Image Classification\n",
        "\n",
        "You can classify images using the cosine similarity (times 100) as the logits to the softmax operation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nqu4GlfPfr-p"
      },
      "outputs": [],
      "source": [
        "from torchvision.datasets import CIFAR100\n",
        "\n",
        "cifar100 = CIFAR100(os.path.expanduser(\"~/.cache\"), transform=preprocess, download=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C4S__zCGy2MT"
      },
      "outputs": [],
      "source": [
        "text_descriptions = [f\"A photo of a {label}\" for label in cifar100.classes]\n",
        "text_tokens = tokenizer.tokenize(text_descriptions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c4z1fm9vCpSR"
      },
      "outputs": [],
      "source": [
        "with torch.no_grad():\n",
        "    text_features = model.encode_text(text_tokens).float()\n",
        "    text_features /= text_features.norm(dim=-1, keepdim=True)\n",
        "\n",
        "text_probs = (100.0 * image_features @ text_features.T).softmax(dim=-1)\n",
        "top_probs, top_labels = text_probs.cpu().topk(5, dim=-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T6Ju_6IBE2Iz"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(16, 16))\n",
        "\n",
        "for i, image in enumerate(original_images):\n",
        "    plt.subplot(4, 4, 2 * i + 1)\n",
        "    plt.imshow(image)\n",
        "    plt.axis(\"off\")\n",
        "\n",
        "    plt.subplot(4, 4, 2 * i + 2)\n",
        "    y = np.arange(top_probs.shape[-1])\n",
        "    plt.grid()\n",
        "    plt.barh(y, top_probs[i])\n",
        "    plt.gca().invert_yaxis()\n",
        "    plt.gca().set_axisbelow(True)\n",
        "    plt.yticks(y, [cifar100.classes[index] for index in top_labels[i].numpy()])\n",
        "    plt.xlabel(\"probability\")\n",
        "\n",
        "plt.subplots_adjust(wspace=0.5)\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Interacting with CLIP.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}